GUIÓN PPT CLOUD 2020-2021

Introdución:

Buenas tardes, somos Alvaro Penalva Alberca, Gonzalo Martinez Bercal, Mario Román Dono y Javier Sotelino Barriga, y vamos a presentar nuestro trabajo de investigación para la asignatura.


Diapo 1: Necesidad de Big Data

El objetivo es ver el apoyo en twitter a ambos candidatos por continente, pais o estado. Un tweet posee una gran cantidad de información que nos ayudará a sacar conclusiones en base a ello.

La necesidad de Big Data en nuestro proyecto viene de la cantidad de tweets producidos durante el periodo de las elecciones de EEUU. Nuestra dataset, se podría considerar pequeña para la cantidad total de tweets. Actualmente disponemos de aproximadamente 2 millones de tweets en total. Necesitamos filtrar, traducir, valorar y clasificar. Es inevitable aplicar tecnicas propias del big data para realizar este proyecto.

Diapo 2: Solución a esta necesidad.

El procesamiento paralelo junto a pandas son los dos grandes pilares en los que nos apoyamos para solucionar estos problemas. 

Pandas, es una herramienta para python que resuelve el problema en alto nivel de procesar un csv con tal cantidad de datos gracias a la creación de dataframes y su manipulación. 

https://pandas.pydata.org/about/

Pandarallel es una extesión basada en la API de Pandas para python, que se encarga de solucionar el problema de rendimiento y tiempo.

https://github.com/nalepae/pandarallel 

Textblob es una herramienta de analisis sentimental basada en Natural Lenguage Tool Kit para python. Está adaptada a alto nivel y viene entrenada para distinguir entre analisis positivos y negativos de una frase, incluso mide la objetividad de la misma. 

https://textblob.readthedocs.io/en/dev/
http://www.nltk.org

Si a todo esto, le sumamos la escabilidad que nos proporciona la nube de amazon, obtenemos conclusiones de nuestros datos en poco tiempo.

Diapo 3: Origen y Modelo de programación

Hemos realizado unos scripts de consola para linux que procesan los csv.

Nuestro acercamiento consiste primero realizar un modelo sin procesamiento paralelo y después adaptarlo. 

El objetivo es ser capaces de leer un csv con los campos requeridos, filtrarlo, traducirlo al inglés, aplicar una valoración (negativa, neutral, o positiva) y clasificarlo según lo buscado en ese script.

El código es sencillo y existe uno para continentes, paises y estados.

En general, hemos ido adaptado nuestro código a las necesidades del software de más peso en nuestro proyecto, las valoraciones.



Diapo 4: Procesamiento paralelo y su aplicación.

Una vez programado un modelo sin procesamiento paralelo, necesitamos adaptarlo para que utilize todos los recursos posibles de nuestro ordenador o de nuestro cluster. Para ello utilizamos pandaralel, una extensión de la api de pandas que divide la ejecución del código en los trabajadores que le mandemos a cambio de un aumento en el tiempo de preparación que se nota en los rendimientos.

No siempre más cores significa más potencia, si revisamos la documentación de pandaralel el incremento de rendimiento está relacionado con el número de cores (físico) de nuestra máquina y el tamaño del archivo. 

Apartir de 10 cores en una máquina de 12, el rendimiento deja de subir y aumenta el tiempo gracias al overhead de preparar la ejecución para 12.


Diapo 5: Descripción del software

¿Por qué Pandas y no SPARK? 

En nuestro proyecto existe una necesidad de utilizar python3 para la traducción de los tweets y para la aplicación del analisis sentimental.
Textbloob y la API de traducción de google (py.translate la adapta a nivel usuario) necesitan de python3 para su rendimiento más optimo. Al spark usar Python2.7 existen errores por ejemplo, al leer csvs y traducirlos.

La API de traducción:

Puesto que Textblob basa sus valoraciones en frases en ingles, existe la necesidad de traducir los tweets. Esto genera un problema de coste, pues mandar 2 millones de peticiones a la api de google translate no es nada barato. 

La api de valoración, textblob.

Textblob basa su ejecución en NLTK (Natural Lenguage Tool Kit), que descompene la frase en palabras, clasifica en verbos, adjetivos, preposiciones... y genera una valoración en función de su orden y peso en la frase.

Diapo 6: Los rendimientos: esto explicarlo viendolo está claro.

Diapo 7: a futuro, es leer la diapo literalemnte pero:

Invertir en los costes de la API de traducción y evitar tratar como neutros los tweets en otro idioma ( aprox el 10% del dataset está en otro idioma)

Agrandar el dataset, actualmente usamos la dataset de 1 semana.

Preparar un entranamiento personalizado para Textblob

Mejorar el modelo de programación.

Si es os ocurre otra cosa ponedlo porfa.


Diapo 8: Conclusiones

En general, las salidas de los programas reflejan la polarización de estas elecciones. Actualmente, sabemos que Biden obtuvo el 51,4 %, lo que se refleja en más apoyo a el por redes sociales que a Trump como vemos en el script de continentes, dónde todos muestran mas simpatía a Joe Biden.

Los votantes de Trump son más activos y con tweets más subjetivos, en general son en su mayoria o extremadamente positivos o extremadamente negativos


--RELLENAR

Diapo 9 : Citas

https://pandas.pydata.org/about/
https://github.com/nalepae/pandarallel 
https://textblob.readthedocs.io/en/dev/
http://www.nltk.org
[....]